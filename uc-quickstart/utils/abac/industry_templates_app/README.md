# ABAC Industry Templates Deployer - Databricks App

A Databricks App that provides a simple UI to deploy ABAC (Attribute-Based Access Control) masking and filtering functions to your Unity Catalog.

## Features

### Complete ABAC Workflow

**Required Steps:**
1. ðŸ”§ **Create Functions** - Deploy masking/filtering UDFs (15 functions)
2. ðŸ·ï¸ **Create Tag Policies** - Define governed tags (4 policies)
3. ðŸ›¡ï¸ **Create ABAC Policies** - Apply column masks and row filters (13 policies)

**Optional Testing Steps:**
4. ðŸ“Š **Create Test Data** - Generate sample tables with _test suffix
5. ðŸ·ï¸ **Tag Test Data** - Apply tags to test table columns (only if test data exists)
6. ðŸ§ª **Test Policies** - Run queries to verify masking (only if test data exists)

### Key Features
- ðŸŽ¯ **Catalog & Schema Selection**: Choose targets via dropdown
- ðŸ­ **Modular Industry Templates**: Easy to add new industries
- ðŸš€ **One-Click Deployment**: Each step executed with a button
- âœ… **Real-time Progress**: Track deployment status
- ðŸ”’ **Conditional Steps**: Test steps only available when test data exists

## What Gets Deployed (Finance Template)

### Step 1: Masking & Filter Functions (15)
**Column Masking Functions (11):**
- `mask_credit_card` - PCI-DSS compliant credit card masking
- `mask_ssn_last4` - SSN protection (last 4 digits)
- `mask_email` - Email masking (hides local part)
- `mask_phone` - Phone number masking (last 4 digits)
- `mask_account_last4` - Account number protection
- `mask_routing_number` - Routing number masking
- `mask_ip_address` - IP address subnet masking
- `mask_transaction_hash` - Transaction ID hashing
- `mask_customer_id_deterministic` - Deterministic customer ID hashing
- `mask_amount_bucket` - Transaction amount bucketing
- `mask_income_bracket` - Income range bucketing

**Row Filter Functions (4):**
- `filter_fraud_flagged_only` - Show only fraudulent transactions
- `filter_high_value_transactions` - Filter transactions > $5K
- `filter_business_hours` - Business hours (9 AM - 5 PM) filter
- `filter_by_region` - Regional access filter (CA example)

### Step 2: Tag Policies (4)
- `pii_type_finance` - PII field types (12 values)
- `pci_compliance_finance` - PCI-DSS compliance
- `data_classification_finance` - Data classification levels
- `fraud_detection_finance` - Fraud flags

### Step 3: ABAC Policies (13)
- 9 column mask policies (credit card, SSN, email, phone, etc.)
- 3 row filter policies (fraud, high-value, business hours)
- All applied to `account users` group

### Step 4-6: Optional Test Data
- 4 test tables with _test suffix
- Sample data for testing
- Tag applications
- Test queries

## How to Deploy

### Prerequisites

1. Databricks workspace with Apps enabled
2. Unity Catalog configured
3. **SQL Warehouse** - App will use a SQL warehouse resource (configured at deployment)
4. Permissions to create functions in target catalog/schema

### Deployment Steps

1. **Create the App in Databricks**:
   - Go to your Databricks workspace
   - Click **+ New** â†’ **App** in the sidebar
   - Under "Create from code", choose **Manual**
   - Name it: `abac-industry-deployer`

2. **Upload App Files**:
   ```bash
   # Navigate to this directory
   cd uc-quickstart/utils/abac/industry_templates_app
   
   # Deploy to Databricks
   databricks apps deploy abac-industry-deployer \
     --source-code-path /Workspace/Users/<your-email>/apps/abac-deployer
   ```

3. **Configure SQL Warehouse Resource**:
   - During app creation, Databricks will prompt you to select a SQL warehouse
   - Choose an existing warehouse or let Databricks provision one
   - The app will automatically use this warehouse (via the `sql_warehouse` resource in `app.yaml`)
   - No need to hard-code warehouse IDs! ðŸŽ‰

4. **Alternative: Use Databricks CLI**:
   ```bash
   # Create workspace directory
   databricks workspace mkdirs /Workspace/Users/<your-email>/apps/abac-deployer
   
   # Upload files
   databricks workspace import-dir . \
     /Workspace/Users/<your-email>/apps/abac-deployer
   
   # Create and start app
   databricks apps create abac-industry-deployer \
     --source-code-path /Workspace/Users/<your-email>/apps/abac-deployer
   ```

## Usage

1. **Open the App**: Navigate to the app URL in your workspace
2. **Select Catalog**: Choose your target catalog from the dropdown
3. **Select Schema**: Choose the schema (or create a new one first)
4. **Select Industry**: Choose "Finance" template
5. **Deploy**: Click "ðŸš€ Deploy Functions" button
6. **Verify**: Check deployment status and function list

## After Deployment

Once functions are deployed, you can:

1. **Create Tables**: Set up your data tables in the same schema
2. **Apply Tags**: Use Unity Catalog tags on sensitive columns
3. **Create ABAC Policies**: Use these functions in your ABAC policies

Example:
```sql
-- Create policy using deployed function
CREATE POLICY mask_ssn ON SCHEMA finance
  COLUMN MASK mask_ssn_last4
  TO `account users`
  FOR TABLES
  MATCH COLUMNS
    hasTagValue('pii_type_finance', 'ssn') AS ssn
  ON COLUMN ssn;
```

## App Structure

```
industry_templates_app/
â”œâ”€â”€ app.py                      # Main Gradio application
â”œâ”€â”€ app.yaml                    # Databricks App configuration
â”œâ”€â”€ README.md                   # This file
â””â”€â”€ industries/                 # Modular industry templates
    â”œâ”€â”€ __init__.py             # Dynamic template loader
    â”œâ”€â”€ finance_template.py     # Finance industry SQL
    â”œâ”€â”€ README.md               # How to add new industries
    â””â”€â”€ <industry>_template.py  # Add more industries here!
```

### Modular Design

Each industry lives in its own file under `industries/` with:
- Functions SQL
- Tag definitions
- ABAC policies
- Test data (optional)
- Tag applications (optional)

**Adding a new industry is simple**: Just create `industries/<industry>_template.py` following the template structure. See `industries/README.md` for details.

## Requirements

- `gradio` - For the web UI
- `databricks-sdk` - For SQL warehouse execution
- SQL Warehouse - For running queries (more scalable than Spark session)

## Architecture

This app follows the **Databricks Apps resource pattern**:

### SQL Warehouse as a Resource
Instead of hard-coding warehouse IDs, the app declares a SQL warehouse as a **resource**:
```yaml
resources:
  - key: sql_warehouse
    type: sql-warehouse
    permission: CAN_USE

env:
  - name: WAREHOUSE_ID
    valueFrom: sql_warehouse  # Auto-inject warehouse ID
```

At runtime, Databricks automatically injects:
- `WAREHOUSE_ID` - The actual warehouse ID
- `DATABRICKS_HOST` - Workspace URL
- `DATABRICKS_CLIENT_ID` - OAuth credentials
- `DATABRICKS_CLIENT_SECRET` - OAuth credentials

The app code just reads `WAREHOUSE_ID` from the environment and never knows the actual ID!

### Benefits over Spark Session
- **Performance**: SQL warehouses are optimized for SQL workloads
- **Resource Management**: Dedicated compute separate from app runtime
- **Scalability**: Auto-scaling capabilities
- **Concurrency**: Better handling of concurrent requests
- **Flexibility**: Swap warehouses without code changes

### References
- [Databricks App Templates](https://github.com/databricks/app-templates)
- [Add SQL Warehouse Resource](https://docs.databricks.com/aws/en/dev-tools/databricks-apps/sql-warehouse)
- [Add Resources to Apps](https://docs.databricks.com/aws/en/dev-tools/databricks-apps/resources)

## Troubleshooting

### App won't start
- Check that `app.yaml` is in the same directory as `app.py`
- Verify the source code path in workspace
- Ensure a SQL warehouse resource is configured for the app

### SQL Warehouse errors
- Check that a warehouse is selected/provisioned for the app
- The warehouse will auto-start if stopped (serverless is fastest)
- Verify you have `CAN USE` permission on the warehouse
- Check `WAREHOUSE_ID` environment variable is being injected (it's automatic)

### Functions fail to create
- Ensure you have `CREATE FUNCTION` permission on the schema
- Verify the catalog and schema exist
- Check that you're not deploying to a system schema
- Verify SQL warehouse has permissions to the catalog/schema

### Can't see catalogs/schemas
- Ensure you have `USE CATALOG` and `USE SCHEMA` permissions
- Check Unity Catalog is enabled in your workspace
- Verify the SQL warehouse can query system tables

## Future Enhancements

- [ ] Add more industry templates (Healthcare, Retail, Manufacturing)
- [ ] Support for table creation
- [ ] Tag application UI
- [ ] ABAC policy creation wizard
- [ ] Deployment history tracking
- [ ] Function testing interface

## Resources

- [Databricks Apps Documentation](https://docs.databricks.com/aws/en/dev-tools/databricks-apps/)
- [Unity Catalog ABAC](https://docs.databricks.com/data-governance/unity-catalog/abac/)
- [Repository](https://github.com/AbhiDatabricks/sandbox)

